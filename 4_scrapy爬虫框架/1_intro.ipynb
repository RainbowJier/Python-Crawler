{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、什么是scrapy框架？\n",
    "    Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。\n",
    "scrapy框架和自己写的爬虫程序的区别就在于：scarpy相当于一个组装好的主机，可以立刻用；自己的爬虫程序就好比组装机，根据自己的需要配置不同的组件。\n",
    "\n",
    "![](picture\\1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、架构概览\n",
    "项目名字\n",
    "* 项目名字\n",
    "    * spiders文件夹(存储的是爬虫文件)\n",
    "        * init\n",
    "        * 自定义的爬虫文件     ***核心功能键***\n",
    "    * init\n",
    "    * items   定义数据结构\n",
    "    * middleware  中间件(代理)\n",
    "    * pipelines   项目管道文件,用于提取Items内容\n",
    "    * settings    配置文件(UA、head。。)\n",
    "## 2.1 Scrapy Engine\n",
    "    引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。\n",
    "此组件相当于爬虫的“大脑”，**是整个爬虫的调度中心**。\n",
    "## 2.2 调度器(Scheduler)\n",
    "    调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。\n",
    "初始的爬取URL和后续在页面中获取的待爬取的URL将放入调度器中，等待爬取。同时调度器会自动去除重复的URL（如果特定的URL不需要去重也可以通过设置实现，如post请求的URL）\n",
    "\n",
    "## 2.3 下载器(Downloader)\n",
    "    下载器负责获取页面数据并提供给引擎，而后提供给spider。\n",
    "\n",
    "## 2.4 Spiders\n",
    "    Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。\n",
    "当页面被爬虫解析所需的数据存入Item后，将被发送到项目管道(Pipeline)，并经过几个特定的次序处理数据，最后存入本地文件或存入数据库。\n",
    "\n",
    "## 2.5 下载器中间件(Downloader middlewares)\n",
    "    下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。\n",
    "通过设置下载器中间件可以实现爬虫自动更换user-agent、IP等功能。\n",
    "## 2.6 Spider中间件(Spider middlewares)\n",
    "    Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。\n",
    "## 2.7 数据流(Data flow)\n",
    "* 引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。\n",
    "\n",
    "* 引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。\n",
    "\n",
    "* 引擎向调度器请求下一个要爬取的URL。\n",
    "\n",
    "* 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。\n",
    "\n",
    "* 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。\n",
    "\n",
    "* 引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。\n",
    "\n",
    "* Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。\n",
    "\n",
    "* 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。\n",
    "\n",
    "* (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、建立Scrapy爬虫项目流程\n",
    "## 3.1 创建爬虫项目\n",
    "* cmd中创建：**scrapy startproject 项目名称**（不允许使用数字开头、不能包含中文）\n",
    "## 3.2 创建爬虫文件\n",
    "* 要在spiders文件夹中去创建爬虫文件：**cd 项目名字\\项目名字\\spiders**\n",
    "* 创建爬虫文件：**scrapy genspider  文件名**\n",
    "## 3.3 运行爬虫代码\n",
    "* **scrapy crawl 爬虫名称**\n",
    "## 3.4 常用指令\n",
    "* scrapy startproject 项目名称：创建一个新项目。\n",
    "* scrapy genspider <爬虫文件名> <域名>：新建爬虫文件。\n",
    "* scrapy runspider <爬虫文件>：运行一个爬虫文件，不需要创建项目。\n",
    "* scrapy crawl <spidername>：运行一个爬虫项目，必须要创建项目。\n",
    "* scrapy list：列出项目中所有爬虫文件。\n",
    "* scrapy view <url地址>：从浏览器中打开 url 地址。\n",
    "* scrapy settings：查看当前项目的配置信息。\n",
    "\n",
    "该命令将会创建包含下列内容的scrapyspider目录:\n",
    "* 项目名字\n",
    "    * spiders文件夹(存储的是爬虫文件)\n",
    "        * init\n",
    "        * 自定义的爬虫文件        ***核心功能键***\n",
    "    * init\n",
    "    * items   定义数据结构\n",
    "    * middleware  中间件(代理)\n",
    "    * pipelines   项目管道文件,用于提取Items内容\n",
    "    * settings    配置文件(UA、head。。)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
